# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19AX87DNmYfT299baOEGtKITvH9KWjkW3
"""

# Lathitha Nongauza - 2615978
import re
import torch
import torch.nn as nn
import numpy as np
import torch.optim as optim
import random
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from numpy.linalg import norm
import statistics as stats
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from torch.utils.data import Dataset, DataLoader, random_split
# the effect of using pretrained or none-pretrained tokens

# -----------------------------
# word2vec
# -----------------------------
class word2vec_network(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(word2vec_network, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, input_size)
        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.1)
        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.1)
        nn.init.constant_(self.fc1.bias, 0.0)
        nn.init.constant_(self.fc2.bias, 0.0)

    def forward(self, x):
        return self.fc2(self.fc1(x))

class word2vec:
    def __init__(self, books, words, k, lr, epoch):
        self.books = books
        self.words = words
        self.k = k
        self.lr = lr
        self.epoch = epoch
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.vocab = []

    def get_embeddings(self, model, word_vector_pairs):
        model.eval()
        embeddings = []
        words = [w for w, _ in word_vector_pairs]

        for _, one_hot in word_vector_pairs:
            one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32).unsqueeze(0)
            with torch.no_grad():
                embedding = model.fc1(one_hot_tensor).squeeze(0).numpy()
            embeddings.append(embedding)

        return np.array(embeddings), words

    def data(self):
        all_sampled_words = []

        for book in self.books:
            try:
                with open(book, "r", encoding='utf-8') as f:
                    corpus = f.read().strip().split()
                    cleaned_words = [re.sub(r'[^\w\s]', '', word.lower()) for word in corpus]

                    if len(cleaned_words) > self.words:
                        sampled_words = random.sample(cleaned_words, self.words)
                    else:
                        sampled_words = cleaned_words

                    all_sampled_words.extend(sampled_words)
            except FileNotFoundError:
                print(f"Warning: File {book} not found. Skipping.")
                continue

        unique_words = list(set(all_sampled_words))
        unique_words.append("<UNK>")
        self.vocab = unique_words
        self.word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
        self.idx_to_word = {idx: word for idx, word in enumerate(unique_words)}

        print(f"Vocabulary size: {len(unique_words)} words")

        vocab_size = len(unique_words)
        one_hot_vectors = np.eye(vocab_size)
        word_vector_pairs = [(word, one_hot_vectors[self.word_to_idx[word]]) for word in unique_words]

        data = []
        for book in self.books:
            try:
                with open(book, "r", encoding='utf-8') as f:
                    corpus = f.read().strip().split()
                    word_array = [re.sub(r'[^\w\s]', '', word.lower()) for word in corpus]

                    for i in range(2, len(word_array)-2):
                        center_word = word_array[i]
                        center_idx = self.word_to_idx.get(center_word, self.word_to_idx["<UNK>"])

                        context_words = [
                            word_array[i-2], word_array[i-1],
                            word_array[i+1], word_array[i+2]
                        ]

                        context_indices = []
                        for context_word in context_words:
                            context_idx = self.word_to_idx.get(context_word, self.word_to_idx["<UNK>"])
                            context_indices.append(context_idx)

                        data.append((center_idx, context_indices))
            except FileNotFoundError:
                continue

        return data, word_vector_pairs

    def train(self):
        lr = self.lr
        numOfEpochs = self.epoch
        negative_samples = self.k

        dataset, word_vector_pairs = self.data()
        vocab_size = len(self.vocab)

        model = word2vec_network(input_size=vocab_size, hidden_size=50)
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.SGD(model.parameters(), lr=lr)

        one_hot_vectors = np.eye(vocab_size)
        word_indices = list(range(vocab_size))

        for epoch in range(numOfEpochs):
            total_loss = 0.0
            random.shuffle(dataset)

            for center_idx, context_indices in dataset:
                # Zero gradients at the start of each sample
                optimizer.zero_grad()

                center_vec = one_hot_vectors[center_idx]
                center_tensor = torch.tensor(center_vec, dtype=torch.float32).unsqueeze(0)

                logits = model(center_tensor).squeeze(0)

                batch_loss = 0.0

                # Process all context words for this center word first
                for target_idx in context_indices:
                    # Positive example
                    pos_score = logits[target_idx].unsqueeze(0)
                    pos_label = torch.tensor([1.0])
                    loss = criterion(pos_score, pos_label)

                    # Negative sampling
                    neg_indices = random.sample(
                        [i for i in word_indices if i != target_idx],
                        min(negative_samples, vocab_size - 1)
                    )

                    if neg_indices:
                        neg_scores = logits[neg_indices]
                        neg_labels = torch.zeros(len(neg_indices))
                        loss += criterion(neg_scores, neg_labels)

                    batch_loss += loss

                # Backward pass once for all context words of this center word
                batch_loss.backward()
                optimizer.step()

                total_loss += batch_loss.item()

            avg_loss = total_loss / len(dataset)
            print(f"Epoch {epoch+1}/{numOfEpochs}, Loss: {avg_loss:.4f}")

        torch.save({
            'model_state_dict': model.state_dict(),
            'word_to_idx': self.word_to_idx,
            'idx_to_word': self.idx_to_word,
            'vocab': self.vocab
        }, 'hp_embeddings.pth')

        return model, word_vector_pairs

# -----------------------------
# CNN Model
# -----------------------------
class CNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, embeddings=None, freeze=True):
        super(CNN, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Use pre-trained embeddings if provided
        if embeddings is not None:
            self.embedding.weight.data.copy_(embeddings)
            if freeze:
                self.embedding.weight.requires_grad = True
        else:
            nn.init.normal_(self.embedding.weight, mean=0.0, std=0.1)

        # Single convolutional layer
        self.conv = nn.Conv2d(1, 500, (3, embedding_dim))
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(500, num_classes)

    def forward(self, x):
        x = self.embedding(x)          # [batch, seq_len, emb_dim]
        x = x.unsqueeze(1)             # [batch, 1, seq_len, emb_dim]

        # Single convolution
        x = F.relu(self.conv(x))       # [batch, 100, seq_len-2, 1]
        x = x.squeeze(3)               # [batch, 100, seq_len-2]

        # Global max pooling
        x = F.max_pool1d(x, x.size(2)) # [batch, 100, 1]
        x = x.squeeze(2)               # [batch, 100]

        x = self.dropout(x)
        return self.fc(x)

# -----------------------------
# Dataset
# -----------------------------
class HPDataset(Dataset):
    def __init__(self, books, word_to_idx):
        self.samples = []
        self.labels = []
        self.word_to_idx = word_to_idx

        for label, book in enumerate(books):
            with open(book, "r", encoding="utf-8") as f:
                text = f.read().strip().split(".")
                for sent in text:
                    tokens = [re.sub(r'[^\w\s]', '', w.lower()) for w in sent.split()]
                    indices = [self.word_to_idx.get(w, self.word_to_idx["<UNK>"]) for w in tokens]
                    if indices:
                        self.samples.append(torch.tensor(indices, dtype=torch.long))
                        self.labels.append(label)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx], self.labels[idx]

def collate_fn(batch):
    texts, labels = zip(*batch)
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)
    labels = torch.tensor(labels, dtype=torch.long)
    return texts_padded, labels

# -----------------------------
# Build vocabulary
# -----------------------------
def build_vocab(books, min_freq=1):
    counter = Counter()
    for book in books:
        with open(book, "r", encoding="utf-8") as f:
            text = f.read().lower()
            words = re.findall(r'\b\w+\b', text)
            counter.update(words)
    vocab = [word for word, freq in counter.items() if freq >= min_freq]
    vocab.append("<UNK>")
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    return word_to_idx

# -----------------------------
# Create Random Embeddings (Non-Pretrained)
# -----------------------------
def create_random_embeddings(vocab_size, embedding_dim=50):
    embeddings = torch.randn(vocab_size, embedding_dim) * 0.1  # mean=0, std=0.1
    return embeddings

# -----------------------------
# Word2Vec embeddings
# -----------------------------
def load_word2vec_embeddings(checkpoint_path, word_to_idx):
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    word2vec_word_to_idx = checkpoint['word_to_idx']
    word2vec_embedding_dim = checkpoint['model_state_dict']['fc1.weight'].shape[0]

    print(f"Word2Vec embedding dim: {word2vec_embedding_dim}")
    embeddings = np.random.uniform(-0.25, 0.25, (len(word_to_idx), word2vec_embedding_dim))

    word2vec_embedding_matrix = checkpoint['model_state_dict']['fc1.weight'].numpy().T

    for word, cnn_idx in word_to_idx.items():
        if word in word2vec_word_to_idx:
            word2vec_idx = word2vec_word_to_idx[word]
            embeddings[cnn_idx] = word2vec_embedding_matrix[word2vec_idx]
            found += 1
        elif word in ["<UNK>", "<PAD>"]:
            continue
    return torch.tensor(embeddings, dtype=torch.float32), word2vec_embedding_dim

# -----------------------------
# Training
# -----------------------------
def train_cnn(books, vocab_size, embedding_dim, num_classes, embeddings, word_to_idx,
              epochs=20, save_path="cnn_hp.pth", patience=3, device="cpu"):
    model = CNN(vocab_size, embedding_dim, num_classes, embeddings).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)

    dataset = HPDataset(books, word_to_idx)
    n_total = len(dataset)
    n_train = int(0.7 * n_total)
    n_val = int(0.15 * n_total)
    n_test = n_total - n_train - n_val
    train_data, val_data, test_data = random_split(dataset, [n_train, n_val, n_test])

    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)
    val_loader   = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn)
    test_loader  = DataLoader(test_data, batch_size=64, shuffle=False, collate_fn=collate_fn)

    best_val_loss = float("inf")
    patience_counter = 0

    for epoch in range(epochs):
        # --- Training ---
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # --- Validation ---
        model.eval()
        val_loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += y_batch.size(0)
                correct += (predicted == y_batch).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total

        print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}")

        # --- Early stopping ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save({
            'model_state_dict': model.state_dict(),
            'word_to_idx': word_to_idx,
            'vocab_size': vocab_size,
            'embedding_dim': embedding_dim,
            'num_classes': num_classes
            }, save_path)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break

    # model.load_state_dict(torch.load(save_path))

    test(model, test_loader, device)

# -----------------------------
# Test
# -----------------------------
def test(model, test_loader, device="cpu"):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    acc = correct / total
    print(f"Test Accuracy: {acc:.4f}")
    return acc

# -----------------------------
# Main
# -----------------------------

if __name__ == "__main__":
    ###########
    # word2vec
    ###########

    # books = word2vec([
    #     "HP1.txt",
    #     "HP2.txt",
    #     "HP3.txt",
    #     "HP4.txt",
    #     "HP5.txt",
    #     "HP6.txt",
    #     "HP7.txt"
    # ], 20_000, 5, 0.1, 30)

    # Train the model
    # print("Training model...")
    # model, word_vector_pairs = books.train()

    ######
    # CNN
    ######

    books_list = ["HP1.txt","HP2.txt","HP3.txt","HP4.txt","HP5.txt","HP6.txt","HP7.txt"]
    word_to_idx = build_vocab(books_list)
    vocab_size = len(word_to_idx)
    embedding_dim = 50
    num_classes = 7

    word2vec_model = "hp_embeddings.pth"
    embeddings, embedding_dim = load_word2vec_embeddings(word2vec_model, word_to_idx)

    train_cnn(books_list, vocab_size, embedding_dim, num_classes, embeddings, word_to_idx, epochs=50)

    ########################
    # Embeddings evaluation
    ########################
    """
    # Get embeddings
    print("Loading model...")
    checkpoint = torch.load("Lab 2/hp_embeddings.pth")
    model = word2vec_network(input_size=len(checkpoint['vocab']), hidden_size=50)
    model.load_state_dict(checkpoint['model_state_dict'])
    # Restore the vocabulary mappings
    books.word_to_idx = checkpoint['word_to_idx']
    books.idx_to_word = checkpoint['idx_to_word']
    books.vocab = checkpoint['vocab']
    # Recreate word_vector_pairs for embedding extraction
    one_hot_vectors = np.eye(len(books.vocab))
    word_vector_pairs = [(word, one_hot_vectors[books.word_to_idx[word]]) for word in books.vocab]

    print("Extracting embeddings...")
    embeddings, words = books.get_embeddings(model, word_vector_pairs)

    # K-means clustering
    num_clusters = 10
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)

    # Dimensionality reduction
    pca = PCA(n_components=2)
    reduced_embeddings = pca.fit_transform(embeddings)

    # Plot
    limit = 200
    limited_embeddings = reduced_embeddings[:limit]
    limited_labels = labels[:limit]
    limited_words = words[:limit]

    plt.figure(figsize=(18, 12))
    scatter = plt.scatter(
        limited_embeddings[:, 0],
        limited_embeddings[:, 1],
        c=limited_labels,
        cmap="tab10",
        alpha=0.7
    )

    for i, word in enumerate(limited_words):
        plt.annotate(word, (limited_embeddings[i, 0], limited_embeddings[i, 1]), fontsize=8, alpha=0.7)

    plt.title("Word Embedding Clusters (PCA) - Limited to 200 Words")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.colorbar(scatter, label="Cluster")
    plt.savefig("word_embedding_clusters.png")
    plt.show()

    #############
    # Evaluation
    #############

    def cosine_similarity(vec1, vec2):
        return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2) + 1e-8)

    related_words_pairs = [
        ("she", "her"), ("he", "him"), ("small", "thin"), ("mrs", "mr"),
        ("boring", "dull"), ("outside", "garden"), ("saw", "stared"),
        ("blonde", "light"), ("steering", "car"), ("opinion", "thought"),
        ("wand", "spell"), ("broomstick", "nimbus"), ("potions", "snape"),
        ("gryffindor", "courage"), ("muggle", "nonmagic"), ("quidditch", "snitch"),
        ("howler", "angry"), ("dementor", "despair"), ("sorting", "hat"),
        ("troll", "club"), ("mirror", "erised"), ("phoenix", "fawkes"),
        ("potion", "polyjuice"), ("herbology", "sprout"), ("scar", "lightning"),
        ("ghost", "nearlyheadless"), ("portkey", "travel"), ("prophecy", "orb"),
        ("werewolf", "lupin"), ("stone", "sorcerer")
    ]

    unrelated_words_pairs = [
        ("she", "dursleys"), ("he", "mysterious"), ("small", "usual"),
        ("mrs", "happily"), ("boring", "owl"), ("outside", "signs"),
        ("saw", "window"), ("blonde", "woke"), ("steering", "fashion"),
        ("opinion", "work"), ("wand", "dursleys"), ("broomstick", "pudding"),
        ("potions", "muggle"), ("gryffindor", "socks"), ("muggle", "broomstick"),
        ("quidditch", "homework"), ("howler", "hagrid"), ("dementor", "butterbeer"),
        ("sorting", "quill"), ("troll", "library"), ("mirror", "kreacher"),
        ("phoenix", "divination"), ("potion", "muggle"), ("herbology", "boring"),
        ("scar", "feast"), ("ghost", "transfiguration"), ("portkey", "sock"),
        ("prophecy", "owl"), ("werewolf", "gillyweed"), ("stone", "quidditch")
    ]

    # Create mapping from word -> embedding
    word_to_embedding = {word: emb for word, emb in zip(words, embeddings)}

    related_scores = []
    unrelated_scores = []

    # Test related pairs
    for w1, w2 in related_words_pairs:
        w1_lower, w2_lower = w1.lower(), w2.lower()
        if w1_lower in word_to_embedding and w2_lower in word_to_embedding:
            sim = cosine_similarity(word_to_embedding[w1_lower], word_to_embedding[w2_lower])
            related_scores.append(sim)

    # Test unrelated pairs
    for w1, w2 in unrelated_words_pairs:
        w1_lower, w2_lower = w1.lower(), w2.lower()
        if w1_lower in word_to_embedding and w2_lower in word_to_embedding:
            sim = cosine_similarity(word_to_embedding[w1_lower], word_to_embedding[w2_lower])
            unrelated_scores.append(sim)

    # Calculate statistics
    if related_scores and unrelated_scores:
        related_mean = stats.mean(related_scores)
        related_var = stats.variance(related_scores) if len(related_scores) > 1 else 0
        unrelated_mean = stats.mean(unrelated_scores)
        unrelated_var = stats.variance(unrelated_scores) if len(unrelated_scores) > 1 else 0

        print("\n=== Results ===")
        print(f"Related pairs mean: {related_mean:.6f}")
        print(f"Related pairs variance: {related_var:.6f}")
        print(f"Unrelated pairs mean: {unrelated_mean:.6f}")
        print(f"Unrelated pairs variance: {unrelated_var:.6f}")

        if related_mean > unrelated_mean:
            print("✓ SUCCESS: Related pairs have higher similarity")
        else:
            print("✗ FAILURE: Unrelated pairs have higher similarity")
    else:
        print("Not enough data to calculate statistics")
    """
############################
# Hand-pick test evaluation
############################
    """
    def plot_prediction_results(predictions, true_labels):

    # Set up the plotting style
    plt.style.use('default')
    sns.set_palette("husl")

    # Create a figure with multiple subplots
    fig = plt.figure(figsize=(20, 16))

    # 1. Confusion Matrix
    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)
    plot_confusion_matrix(predictions, true_labels, ax1)

    plt.tight_layout()
    plt.savefig('pre_prediction_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_confusion_matrix(predictions, true_labels, ax):

    confusion = np.zeros((7, 7), dtype=int)
    for pred in predictions:
        true_idx = pred['true_book']
        pred_idx = pred['predicted_book']
        confusion[true_idx][pred_idx] += 1

    book_names = [f"Book {i+1}" for i in range(7)]

    sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues',
                xticklabels=book_names, yticklabels=book_names, ax=ax)
    ax.set_title('Confusion Matrix\n(Pretrained)', fontsize=14, fontweight='bold')
    ax.set_xlabel('Predicted Book', fontsize=12)
    ax.set_ylabel('True Book', fontsize=12)
    ax.tick_params(axis='x', rotation=45)
    ax.tick_params(axis='y', rotation=0)



def predict_book(sentences, model, word_to_idx, device="cpu", true_labels=None):
    # Ensure sentences is a list
    if isinstance(sentences, str):
        sentences = [sentences]

    model.eval()  # Set model to evaluation mode

    # Preprocess sentences
    processed_samples = []
    for sentence in sentences:
        # Clean and tokenize
        tokens = [re.sub(r'[^\w\s]', '', w.lower()) for w in sentence.split()]
        # Convert to indices
        indices = [word_to_idx.get(w, word_to_idx["<UNK>"]) for w in tokens]
        if indices:
            processed_samples.append(torch.tensor(indices, dtype=torch.long))

    if not processed_samples:
        return []

    # Pad sequences
    texts_padded = pad_sequence(processed_samples, batch_first=True, padding_value=0)
    texts_padded = texts_padded.to(device)

    # Make predictions
    with torch.no_grad():
        outputs = model(texts_padded)
        probabilities = F.softmax(outputs, dim=1)
        _, predicted = torch.max(outputs, 1)

    # Convert to readable results
    predictions = []
    for i, (pred, prob) in enumerate(zip(predicted, probabilities)):
        prediction_data = {
            'sentence': sentences[i],
            'predicted_book': pred.item(),
            'probability': prob[pred].item(),
            'all_probabilities': prob.cpu().numpy()
        }

        # Add true label if provided
        if true_labels is not None and i < len(true_labels):
            prediction_data['true_book'] = true_labels[i]
            prediction_data['correct'] = (pred.item() == true_labels[i])

        predictions.append(prediction_data)

    return predictions

# Helper function to get book names
def get_book_name(book_idx):
    Map book index to book name
    book_names = {
        0: "Harry Potter and the Philosopher's Stone",
        1: "Harry Potter and the Chamber of Secrets",
        2: "Harry Potter and the Prisoner of Azkaban",
        3: "Harry Potter and the Goblet of Fire",
        4: "Harry Potter and the Order of the Phoenix",
        5: "Harry Potter and the Half-Blood Prince",
        6: "Harry Potter and the Deathly Hallows"
    }
    return book_names.get(book_idx, "Unknown Book")

# Example usage function with correct book labels
def example_usage(model, word_to_idx, device):
    Extended example with 2 longer texts from each of the 7 books (14 total)

    # Two longer text excerpts from each Harry Potter book
    test_texts = [
        # Book 1: Philosopher's Stone (2 excerpts)
        "Harry Potter had lived with the Dursleys for ten years, ever since he had been a baby and his parents had died in that car crash. He was small and skinny for his age, with thin legs and arms, and he wore round glasses held together with tape because the Dursleys never bought him new ones. He had a thin face, untidy black hair, and bright green eyes. The only thing Harry liked about his own appearance was a very thin scar on his forehead which was shaped like a bolt of lightning.",

        "Hagrid took out a bunch of keys, unlocked the door, and ushered Harry inside. The hut was filled with the smell of wood smoke and bacon. A massive wooden bed stood in one corner, and in front of the fireplace was a table laid with enough food for six people. Harry sat down on a stool by the fire while Hagrid busied himself with the copper kettle. 'Couldn't make us a cup o' tea, could yeh? I'm parched.'",

        # Book 2: Chamber of Secrets (2 excerpts)
        "The Dursleys' house on Privet Drive was exactly as Harry remembered it: impeccably neat and tidy, with a perfectly manicured lawn and not a single flower out of place. The only sign that anything unusual had ever happened there was the cat flap installed in the front door, through which Aunt Petunia had been pushing letters to Harry all summer. Harry's room looked exactly as he had left it, except that his school trunk was now locked and his wand was hidden under a loose floorboard.",

        "The polyjuice potion was bubbling away in Moaning Myrtle's bathroom, giving off a smell like overcooked cabbage. Hermione had been tending to it for weeks, adding ingredients at precisely the right moments. 'The transformation will only last for an hour,' she warned them, 'so we'll have to work quickly. Remember, we're trying to find out if Malfoy is the heir of Slytherin, not to cause trouble.'",

        # Book 3: Prisoner of Azkaban (2 excerpts)
        "The Knight Bus arrived with a deafening BANG, its purple exterior gleaming under the streetlights. The conductor, Stan Shunpike, helped Harry aboard while explaining the emergency transport service for stranded witches and wizards. The bus was furnished with brass bedsteads and chandeliers, and it hurtled through the night, squeezing between oncoming cars and narrowly missing lampposts as it delivered Harry to the Leaky Cauldron.",

        "Professor Lupin's boggart lesson was one of the most practical Defense Against the Dark Arts classes Harry had ever attended. Instead of just reading from textbooks, Lupin taught them how to confront their deepest fears. When it was Harry's turn, the boggart transformed into a dementor, but Lupin stepped in front of him, causing it to become a floating silvery orb. 'The charm is simple, Harry,' Lupin explained. 'Think of your happiest memory.'",

        # Book 4: Goblet of Fire (2 excerpts)
        "The Quidditch World Cup stadium was enormous, with towering golden goalposts and enough seating for a hundred thousand witches and wizards. The pre-match atmosphere was electric, with vendors selling souvenirs and magical food. The Irish team's mascots were leprechauns that showered the crowd with gold coins, while the Bulgarian team was supported by veela whose dance made every male spectator temporarily lose their senses.",

        "The first task of the Triwizard Tournament was even more terrifying than Harry had imagined. He stood facing a Hungarian Horntail, one of the most dangerous dragon breeds, which was guarding a clutch of eggs including the golden egg he needed to retrieve. His plan was to use the summoning charm to call his broomstick, hoping he could outfly the dragon long enough to grab the egg without being burned to a crisp.",

        # Book 5: Order of the Phoenix (2 excerpts)
        "Number twelve, Grimmauld Place revealed itself between numbers eleven and thirteen as Harry read Sirius's note. The house was dark and gloomy, with peeling wallpaper and the smell of dust and neglect. The portrait of Sirius's mother screamed insults at anyone who passed, and the house was filled with dark artifacts that had to be carefully handled. This was the new headquarters of the Order of the Phoenix, a secret organization fighting against Voldemort's return.",

        "The Room of Requirement became the secret training ground for Dumbledore's Army. When Harry needed a place to teach defensive magic, the room provided everything they needed: practice dummies, cushions for falling, and even a library of defensive spell books. Under Harry's instruction, the students learned to cast patronuses, perform shield charms, and disarm opponents, preparing themselves for the dark times they knew were coming.",

        # Book 6: Half-Blood Prince (2 excerpts)
        "The potions textbook Harry found was unlike any he had ever seen. It was filled with handwritten notes that improved upon the standard instructions, creating more powerful potions with fewer steps. The mysterious 'Half-Blood Prince' had annotated every page, and following these instructions made Harry the best potioneer in the class, much to Snape's annoyance and Hermione's frustration.",

        "The cave where Voldemort had hidden a horcrux was accessible only at low tide and protected by powerful dark magic. As Harry and Dumbledore approached, they had to cut their hands and smear blood on the rock wall to reveal the entrance. Inside, they found an underground lake with a small island in the center, where a glowing green potion protected the locket horcrux. Dumbledore had to drink the entire potion, which caused him terrible pain and weakness.",

        # Book 7: Deathly Hallows (2 excerpts)
        "The break-in at Gringotts was one of the most daring missions Harry had ever attempted. Disguised as Bellatrix Lestrange, Hermione led them into the wizarding bank, where they had to navigate past suspicious goblins and various security measures to reach Bellatrix's vault. The vault itself was protected by a multiplying charm that caused any touched treasure to replicate endlessly, nearly burying them alive as they searched for Hufflepuff's cup.",

        "The final battle of Hogwarts raged through the castle and grounds. Death Eaters had breached the defenses, and students, teachers, and members of the Order of the Phoenix fought desperately to protect the school. Harry moved through the chaos, searching for Voldemort while avoiding curses and helping fallen friends. The Great Hall became a makeshift hospital wing, and the casualties mounted as the battle reached its terrible climax."
    ]

    # True book indices (0-6 for the 7 books, repeated for the 2 excerpts each)
    true_labels = [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]

    print("Extended Prediction Test: 2 excerpts from each of the 7 Harry Potter books")
    print("=" * 100)
    print(f"Total test texts: {len(test_texts)}")
    print("=" * 100)

    predictions = predict_book(test_texts, model, word_to_idx, device, true_labels)

    # Group by book for better analysis
    book_results = {i: {'correct': 0, 'total': 0, 'avg_confidence': 0} for i in range(7)}
    correct_count = 0

    for i, pred in enumerate(predictions):
        predicted_name = get_book_name(pred['predicted_book'])
        true_name = get_book_name(pred['true_book'])
        true_book_idx = pred['true_book']

        status = "✓" if pred['correct'] else "✗"
        confidence = pred['probability']

        # Update book-specific statistics
        book_results[true_book_idx]['total'] += 1
        book_results[true_book_idx]['avg_confidence'] += confidence
        if pred['correct']:
            book_results[true_book_idx]['correct'] += 1
            correct_count += 1

        print(f"{status} Book {true_book_idx + 1}: {true_name}")
        print(f"   Prediction: {predicted_name} (confidence: {confidence:.3f})")
        print(f"   Excerpt: '{pred['sentence'][:80]}...'")
        print("-" * 80)

    # Calculate overall statistics
    accuracy = correct_count / len(predictions)

    print("\n" + "=" * 100)
    print("SUMMARY RESULTS BY BOOK")
    print("=" * 100)

    for book_idx in range(7):
        book_name = get_book_name(book_idx)
        correct = book_results[book_idx]['correct']
        total = book_results[book_idx]['total']
        avg_conf = book_results[book_idx]['avg_confidence'] / total if total > 0 else 0

        book_accuracy = correct / total if total > 0 else 0
        print(f"Book {book_idx + 1}: {book_name}")
        print(f"   Accuracy: {correct}/{total} ({book_accuracy:.2%})")
        print(f"   Avg Confidence: {avg_conf:.3f}")
        print()

    print("=" * 100)
    print(f"OVERALL ACCURACY: {correct_count}/{len(predictions)} ({accuracy:.2%})")
    print("=" * 100)

    # Show confusion matrix
    print("\nCONFUSION ANALYSIS:")
    confusion = np.zeros((7, 7), dtype=int)
    for pred in predictions:
        true_idx = pred['true_book']
        pred_idx = pred['predicted_book']
        confusion[true_idx][pred_idx] += 1

    print("True→Predicted matrix:")
    print("   0 1 2 3 4 5 6")
    for i in range(7):
        row = " ".join(f"{count:2d}" for count in confusion[i])
        print(f"{i} {row}")
    plot_prediction_results(predictions, true_labels)
    return predictions

# Function to make predictions with correct book comparison
def predict_with_correctness(text, true_book_idx, model_path, device="cpu"):

    model, word_to_idx = load_model_for_predictions(model_path, device)

    if isinstance(text, str):
        text = [text]
    if isinstance(true_book_idx, int):
        true_book_idx = [true_book_idx]

    predictions = predict_book(text, model, word_to_idx, device, true_book_idx)

    # Print results
    for i, pred in enumerate(predictions):
        predicted_name = get_book_name(pred['predicted_book'])
        true_name = get_book_name(pred['true_book'])
        status = "✓" if pred['correct'] else "✗"

        print(f"{status} '{pred['sentence'][:50]}...'")
        print(f"   True: {true_name}")
        print(f"   Pred: {predicted_name} ({pred['probability']:.3f})")
        print()

    return predictions

# Modified main function to include prediction capability
if __name__ == "__main__":
    books_list = ["HP1.txt","HP2.txt","HP3.txt","HP4.txt","HP5.txt","HP6.txt","HP7.txt"]
    word_to_idx = build_vocab(books_list)
    vocab_size = len(word_to_idx)
    embedding_dim = 50
    num_classes = 7

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    print(f"Vocabulary size: {vocab_size}")
    model = CNN(vocab_size, embedding_dim, num_classes)
    # Example of loading and using for prediction
    print("\nLoading model for prediction...")
    checkpoint = torch.load('cnn_hp.pth', map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    word_to_idx = checkpoint['word_to_idx']

    # Run the extended test
    example_usage(model, word_to_idx, device)

    # Example of single prediction with correctness check
    print("\n" + "="*80)
    print("Single Prediction Example:")
    print("="*80)

    test_sentence = "Harry saw the giant three-headed dog for the first time"
    true_book = 0  # Philosopher's Stone

    result = predict_with_correctness(test_sentence, true_book, 'cnn_hp.pth', device)
    """

# Lathitha Nongauza - 2615978
import re
import torch
import torch.nn as nn
import numpy as np
import torch.optim as optim
import random
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from numpy.linalg import norm
import statistics as stats
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from torch.utils.data import Dataset, DataLoader, random_split
# the effect of using pretrained or none-pretrained tokens

# -----------------------------
# word2vec
# -----------------------------
class word2vec_network(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(word2vec_network, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, input_size)
        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.1)
        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.1)
        nn.init.constant_(self.fc1.bias, 0.0)
        nn.init.constant_(self.fc2.bias, 0.0)

    def forward(self, x):
        return self.fc2(self.fc1(x))

class word2vec:
    def __init__(self, books, words, k, lr, epoch):
        self.books = books
        self.words = words
        self.k = k
        self.lr = lr
        self.epoch = epoch
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.vocab = []

    def get_embeddings(self, model, word_vector_pairs):
        model.eval()
        embeddings = []
        words = [w for w, _ in word_vector_pairs]

        for _, one_hot in word_vector_pairs:
            one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32).unsqueeze(0)
            with torch.no_grad():
                embedding = model.fc1(one_hot_tensor).squeeze(0).numpy()
            embeddings.append(embedding)

        return np.array(embeddings), words

    def data(self):
        all_sampled_words = []

        for book in self.books:
            try:
                with open(book, "r", encoding='utf-8') as f:
                    corpus = f.read().strip().split()
                    cleaned_words = [re.sub(r'[^\w\s]', '', word.lower()) for word in corpus]

                    if len(cleaned_words) > self.words:
                        sampled_words = random.sample(cleaned_words, self.words)
                    else:
                        sampled_words = cleaned_words

                    all_sampled_words.extend(sampled_words)
            except FileNotFoundError:
                print(f"Warning: File {book} not found. Skipping.")
                continue

        unique_words = list(set(all_sampled_words))
        unique_words.append("<UNK>")
        self.vocab = unique_words
        self.word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
        self.idx_to_word = {idx: word for idx, word in enumerate(unique_words)}

        print(f"Vocabulary size: {len(unique_words)} words")

        vocab_size = len(unique_words)
        one_hot_vectors = np.eye(vocab_size)
        word_vector_pairs = [(word, one_hot_vectors[self.word_to_idx[word]]) for word in unique_words]

        data = []
        for book in self.books:
            try:
                with open(book, "r", encoding='utf-8') as f:
                    corpus = f.read().strip().split()
                    word_array = [re.sub(r'[^\w\s]', '', word.lower()) for word in corpus]

                    for i in range(2, len(word_array)-2):
                        center_word = word_array[i]
                        center_idx = self.word_to_idx.get(center_word, self.word_to_idx["<UNK>"])

                        context_words = [
                            word_array[i-2], word_array[i-1],
                            word_array[i+1], word_array[i+2]
                        ]

                        context_indices = []
                        for context_word in context_words:
                            context_idx = self.word_to_idx.get(context_word, self.word_to_idx["<UNK>"])
                            context_indices.append(context_idx)

                        data.append((center_idx, context_indices))
            except FileNotFoundError:
                continue

        return data, word_vector_pairs

    def train(self):
        lr = self.lr
        numOfEpochs = self.epoch
        negative_samples = self.k

        dataset, word_vector_pairs = self.data()
        vocab_size = len(self.vocab)

        model = word2vec_network(input_size=vocab_size, hidden_size=50)
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.SGD(model.parameters(), lr=lr)

        one_hot_vectors = np.eye(vocab_size)
        word_indices = list(range(vocab_size))

        for epoch in range(numOfEpochs):
            total_loss = 0.0
            random.shuffle(dataset)

            for center_idx, context_indices in dataset:
                # Zero gradients at the start of each sample
                optimizer.zero_grad()

                center_vec = one_hot_vectors[center_idx]
                center_tensor = torch.tensor(center_vec, dtype=torch.float32).unsqueeze(0)

                logits = model(center_tensor).squeeze(0)

                batch_loss = 0.0

                # Process all context words for this center word first
                for target_idx in context_indices:
                    # Positive example
                    pos_score = logits[target_idx].unsqueeze(0)
                    pos_label = torch.tensor([1.0])
                    loss = criterion(pos_score, pos_label)

                    # Negative sampling
                    neg_indices = random.sample(
                        [i for i in word_indices if i != target_idx],
                        min(negative_samples, vocab_size - 1)
                    )

                    if neg_indices:
                        neg_scores = logits[neg_indices]
                        neg_labels = torch.zeros(len(neg_indices))
                        loss += criterion(neg_scores, neg_labels)

                    batch_loss += loss

                # Backward pass once for all context words of this center word
                batch_loss.backward()
                optimizer.step()

                total_loss += batch_loss.item()

            avg_loss = total_loss / len(dataset)
            print(f"Epoch {epoch+1}/{numOfEpochs}, Loss: {avg_loss:.4f}")

        torch.save({
            'model_state_dict': model.state_dict(),
            'word_to_idx': self.word_to_idx,
            'idx_to_word': self.idx_to_word,
            'vocab': self.vocab
        }, 'hp_embeddings.pth')

        return model, word_vector_pairs

# -----------------------------
# CNN Model
# -----------------------------
class CNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, embeddings, freeze=True):
        super(CNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.embedding.weight.data.copy_(embeddings)
        if freeze:
            self.embedding.weight.requires_grad = True

        # Convolutional layers - adapt to the actual embedding dimension
        self.convs = nn.ModuleList([
            nn.Conv2d(1, 100, (k, embedding_dim)) for k in [3,4,5]
        ])
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(100 * len([3,4,5]), num_classes)

    def forward(self, x):
        x = self.embedding(x)          # [batch, seq_len, emb_dim]
        x = x.unsqueeze(1)             # [batch, 1, seq_len, emb_dim]
        convs = [F.relu(conv(x)).squeeze(3) for conv in self.convs]
        pools = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in convs]
        out = torch.cat(pools, 1)
        out = self.dropout(out)
        return self.fc(out)

# -----------------------------
# Dataset
# -----------------------------
class HPDataset(Dataset):
    def __init__(self, books, word_to_idx):
        self.samples = []
        self.labels = []
        self.word_to_idx = word_to_idx

        for label, book in enumerate(books):
            with open(book, "r", encoding="utf-8") as f:
                text = f.read().strip().split(".")
                for sent in text:
                    tokens = [re.sub(r'[^\w\s]', '', w.lower()) for w in sent.split()]
                    indices = [self.word_to_idx.get(w, self.word_to_idx["<UNK>"]) for w in tokens]
                    if indices:
                        self.samples.append(torch.tensor(indices, dtype=torch.long))
                        self.labels.append(label)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx], self.labels[idx]

def collate_fn(batch):
    texts, labels = zip(*batch)
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)
    labels = torch.tensor(labels, dtype=torch.long)
    return texts_padded, labels

# -----------------------------
# Build vocabulary
# -----------------------------
def build_vocab(books, min_freq=1):
    counter = Counter()
    for book in books:
        with open(book, "r", encoding="utf-8") as f:
            text = f.read().lower()
            words = re.findall(r'\b\w+\b', text)
            counter.update(words)
    vocab = [word for word, freq in counter.items() if freq >= min_freq]
    vocab.append("<UNK>")
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    return word_to_idx

# -----------------------------
# Load GloVe embeddings
# -----------------------------
def load_glove_embeddings(glove_file, word_to_idx, embedding_dim=50):
    embeddings = np.random.uniform(-0.25, 0.25, (len(word_to_idx), embedding_dim))
    print("Loading GloVe embeddings...")

    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            split_line = line.strip().split()
            if len(split_line) != embedding_dim + 1:  # word + vector
                continue  # skip malformed lines
            word = split_line[0]
            try:
                vector = np.array(split_line[1:], dtype=np.float32)
            except ValueError:
                continue  # skip lines that cannot be converted to float
            if word in word_to_idx:
                embeddings[word_to_idx[word]] = vector

    return torch.tensor(embeddings, dtype=torch.float32)

# -----------------------------
# Use trained Word2Vec embeddings
# -----------------------------
def load_word2vec_embeddings(checkpoint_path, word_to_idx):
    """
    Load embeddings from a trained Word2Vec model checkpoint

    Args:
        checkpoint_path: Path to the saved Word2Vec model (.pth file)
        word_to_idx: Current vocabulary mapping for CNN
    """
    # Load the Word2Vec checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # Get Word2Vec vocabulary and embeddings
    word2vec_word_to_idx = checkpoint['word_to_idx']
    word2vec_embedding_dim = checkpoint['model_state_dict']['fc1.weight'].shape[0]  # Get actual embedding dim

    print(f"Word2Vec embedding dim: {word2vec_embedding_dim}")

    # Initialize embeddings with random values for current CNN vocabulary
    # Use the SAME dimension as Word2Vec embeddings
    embeddings = np.random.uniform(-0.25, 0.25, (len(word_to_idx), word2vec_embedding_dim))

    found = 0

    # Extract the embedding matrix from Word2Vec model
    # fc1.weight has shape [hidden_size, vocab_size] - we need to transpose it
    word2vec_embedding_matrix = checkpoint['model_state_dict']['fc1.weight'].numpy().T

    # Map words from current vocabulary to Word2Vec embeddings
    for word, cnn_idx in word_to_idx.items():
        if word in word2vec_word_to_idx:
            # Get the index in the Word2Vec vocabulary
            word2vec_idx = word2vec_word_to_idx[word]
            # Copy the embedding (word2vec_embedding_matrix is [vocab_size, embedding_dim])
            embeddings[cnn_idx] = word2vec_embedding_matrix[word2vec_idx]
            found += 1
        elif word in ["<UNK>", "<PAD>"]:
            # Keep random initialization for special tokens
            continue

    print(f"Found {found}/{len(word_to_idx)} words in Word2Vec embeddings")
    return torch.tensor(embeddings, dtype=torch.float32), word2vec_embedding_dim

# -----------------------------
# Training
# -----------------------------
def train_cnn(books, vocab_size, embedding_dim, num_classes, embeddings, word_to_idx,
              epochs=20, save_path="cnn_hp.pth", patience=3, device="cpu"):
    model = CNN(vocab_size, embedding_dim, num_classes, embeddings).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)

    dataset = HPDataset(books, word_to_idx)
    n_total = len(dataset)
    n_train = int(0.7 * n_total)
    n_val = int(0.15 * n_total)
    n_test = n_total - n_train - n_val
    train_data, val_data, test_data = random_split(dataset, [n_train, n_val, n_test])

    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)
    val_loader   = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_fn)
    test_loader  = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_fn)

    best_val_loss = float("inf")
    patience_counter = 0

    for epoch in range(epochs):
        # --- Training ---
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # --- Validation ---
        model.eval()
        val_loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += y_batch.size(0)
                correct += (predicted == y_batch).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total

        print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}")

        # --- Early stopping ---
        # if avg_val_loss < best_val_loss:
        #     best_val_loss = avg_val_loss
        #     patience_counter = 0
        #     torch.save(model.state_dict(), save_path)  # save best model
        # else:
        #     patience_counter += 1
        #     if patience_counter >= patience:
        #         print("Early stopping triggered.")
        #         break

    # --- Load best model and test ---
    # model.load_state_dict(torch.load(save_path))
    test(model, test_loader, device)

# -----------------------------
# Test
# -----------------------------
def test(model, test_loader, device="cpu"):
    """
    Evaluate a trained CNN on a test dataset.

    Args:
        model: trained CNN model (torch.nn.Module)
        test_loader: DataLoader for test dataset
        device: "cpu" or "cuda"
    Returns:
        accuracy (float)
    """
    model.eval()  # evaluation mode
    correct, total = 0, 0

    with torch.no_grad():  # no gradients needed
        for inputs, labels in test_loader:
            # Move data to device
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)

            # Get predicted class (highest logit)
            _, predicted = torch.max(outputs, 1)

            # Update counts
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    acc = correct / total
    print(f"Test Accuracy: {acc:.4f}")
    return acc

# -----------------------------
# Main
# -----------------------------

if __name__ == "__main__":
    ###########
    # word2vec
    ###########

    books = word2vec([
        "HP1.txt",
        "HP2.txt",
        "HP3.txt",
        "HP4.txt",
        "HP5.txt",
        "HP6.txt",
        "HP7.txt"
    ], 20_000, 5, 0.1, 30)

    # Train the model (commented out as user wants to use existing weights)
    # print("Training model...")
    # model, word_vector_pairs = books.train()

    ######
    # CNN
    ######

    books_list = ["HP1.txt","HP2.txt","HP3.txt","HP4.txt","HP5.txt","HP6.txt","HP7.txt"]
    word_to_idx = build_vocab(books_list)
    vocab_size = len(word_to_idx)
    embedding_dim = 50
    num_classes = 7

    word2vec_model_path = "nbc_embeddings.pth" # Path to the model weights file
    glove_file = "glove.6B.50d.txt"

    # Manually load model weights and extract embeddings
    # Determine the vocabulary size used when nbc_embeddings.pth was trained
    # Based on the previous error, it was likely 9795
    nbc_vocab_size = 9795
    nbc_embedding_dim = 50 # Assuming the hidden size was 50

    # Create a dummy word2vec_network model with the original vocabulary size
    word2vec_model = word2vec_network(input_size=nbc_vocab_size, hidden_size=nbc_embedding_dim)

    # Load the state dictionary
    state_dict = torch.load(word2vec_model_path, map_location='cpu')

    # Check if the state_dict is nested or contains only the weights
    if 'model_state_dict' in state_dict:
        # If nested (like hp_embeddings.pth), load the nested state_dict
        word2vec_model.load_state_dict(state_dict['model_state_dict'])
        # If vocabulary mapping was also saved, use it here (if needed for mapping)
        # word2vec_word_to_idx = state_dict.get('word_to_idx', None)
    else:
        # If it's just the weights, load them directly
        word2vec_model.load_state_dict(state_dict)
        # In this case, we don't have the original word_to_idx mapping from nbc_embeddings.pth
        # We will need to rely on the mapping within load_word2vec_embeddings

    # Extract the embedding matrix from the loaded model
    # The embedding matrix is the weight matrix of the first linear layer (fc1)
    # Its shape is [hidden_size, input_size], we need to transpose it to [input_size, hidden_size]
    nbc_embedding_matrix = word2vec_model.fc1.weight.data.T.numpy()

    print(f"Loaded NBC embedding dim: {nbc_embedding_dim}")
    print(f"Loaded NBC vocabulary size (from weights): {nbc_embedding_matrix.shape[0]}")


    # Now, create the embeddings for the CNN's vocabulary
    # Initialize with random values for the current CNN vocabulary size
    embeddings = np.random.uniform(-0.25, 0.25, (vocab_size, nbc_embedding_dim))

    # We need a way to map words from the CNN's vocabulary to the NBC embeddings
    # Since nbc_embeddings.pth doesn't have word_to_idx, we'll assume a simple 1-to-1 mapping
    # based on index for the words that were in the original NBC vocabulary.
    # This is a strong assumption and might not be correct if the vocabularies were different.
    # A better approach would require the original word_to_idx from the NBC training.
    # However, given the constraint of only having weights, we'll proceed with this assumption.

    # Create a dummy word_to_idx for the NBC embeddings based on index order
    nbc_word_to_idx_dummy = {idx: idx for idx in range(nbc_vocab_size)} # This is likely incorrect without the actual vocab

    found = 0
    # Map words from current CNN vocabulary to NBC embeddings based on index
    # This part is problematic without the actual NBC vocabulary mapping
    # We'll iterate through the CNN vocabulary and if an index is within the range
    # of the NBC embeddings, we'll use that embedding. This is a fallback.
    for word, cnn_idx in word_to_idx.items():
        # This mapping is incorrect without the actual NBC vocabulary
        # We are assuming the index in the CNN vocabulary corresponds to the index in the NBC embeddings
        # if cnn_idx < nbc_vocab_size:
        #     embeddings[cnn_idx] = nbc_embedding_matrix[cnn_idx]
        #     found += 1
        # A more robust approach would require the original nbc_word_to_idx
        pass # We will skip direct mapping here and rely on the random init for now

    print(f"Mapped {found}/{vocab_size} words from CNN vocab to NBC embeddings (this mapping is likely incorrect)")
    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)
    embedding_dim = nbc_embedding_dim # Update embedding_dim to match the loaded weights

    # embeddings, embedding_dim = load_word2vec_embeddings(word2vec_model_path, word_to_idx) # This function cannot be used

    train_cnn(books_list, vocab_size, embedding_dim, num_classes, embeddings_tensor, word_to_idx, epochs=50)

    ########################
    # Embeddings evaluation
    ########################
    """
    # Get embeddings
    print("Loading model...")
    checkpoint = torch.load("Lab 2/hp_embeddings.pth")
    model = word2vec_network(input_size=len(checkpoint['vocab']), hidden_size=50)
    model.load_state_dict(checkpoint['model_state_dict'])
    # Restore the vocabulary mappings
    books.word_to_idx = checkpoint['word_to_idx']
    books.idx_to_word = checkpoint['idx_to_word']
    books.vocab = checkpoint['vocab']
    # Recreate word_vector_pairs for embedding extraction
    one_hot_vectors = np.eye(len(books.vocab))
    word_vector_pairs = [(word, one_hot_vectors[books.word_to_idx[word]]) for word in books.vocab]

    print("Extracting embeddings...")
    embeddings, words = books.get_embeddings(model, word_vector_pairs)

    # K-means clustering
    num_clusters = 10
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)

    # Dimensionality reduction
    pca = PCA(n_components=2)
    reduced_embeddings = pca.fit_transform(embeddings)

    # Plot
    limit = 200
    limited_embeddings = reduced_embeddings[:limit]
    limited_labels = labels[:limit]
    limited_words = words[:limit]

    plt.figure(figsize=(18, 12))
    scatter = plt.scatter(
        limited_embeddings[:, 0],
        limited_embeddings[:, 1],
        c=limited_labels,
        cmap="tab10",
        alpha=0.7
    )

    for i, word in enumerate(limited_words):
        plt.annotate(word, (limited_embeddings[i, 0], limited_embeddings[i, 1]), fontsize=8, alpha=0.7)

    plt.title("Word Embedding Clusters (PCA) - Limited to 200 Words")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.colorbar(scatter, label="Cluster")
    plt.savefig("word_embedding_clusters.png")
    plt.show()

    #############
    # Evaluation
    #############

    def cosine_similarity(vec1, vec2):
        return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2) + 1e-8)

    related_words_pairs = [
        ("she", "her"), ("he", "him"), ("small", "thin"), ("mrs", "mr"),
        ("boring", "dull"), ("outside", "garden"), ("saw", "stared"),
        ("blonde", "light"), ("steering", "car"), ("opinion", "thought"),
        ("wand", "spell"), ("broomstick", "nimbus"), ("potions", "snape"),
        ("gryffindor", "courage"), ("muggle", "nonmagic"), ("quidditch", "snitch"),
        ("howler", "angry"), ("dementor", "despair"), ("sorting", "hat"),
        ("troll", "club"), ("mirror", "erised"), ("phoenix", "fawkes"),
        ("potion", "polyjuice"), ("herbology", "sprout"), ("scar", "lightning"),
        ("ghost", "nearlyheadless"), ("portkey", "travel"), ("prophecy", "orb"),
        ("werewolf", "lupin"), ("stone", "sorcerer")
    ]

    unrelated_words_pairs = [
        ("she", "dursleys"), ("he", "mysterious"), ("small", "usual"),
        ("mrs", "happily"), ("boring", "owl"), ("outside", "signs"),
        ("saw", "window"), ("blonde", "woke"), ("steering", "fashion"),
        ("opinion", "work"), ("wand", "dursleys"), ("broomstick", "pudding"),
        ("potions", "muggle"), ("gryffindor", "socks"), ("muggle", "broomstick"),
        ("quidditch", "homework"), ("howler", "hagrid"), ("dementor", "butterbeer"),
        ("sorting", "quill"), ("troll", "library"), ("mirror", "kreacher"),
        ("phoenix", "divination"), ("potion", "muggle"), ("herbology", "boring"),
        ("scar", "feast"), ("ghost", "transfiguration"), ("portkey", "sock"),
        ("prophecy", "owl"), ("werewolf", "gillyweed"), ("stone", "quidditch")
    ]

    # Create mapping from word -> embedding
    word_to_embedding = {word: emb for word, emb in zip(words, embeddings)}

    related_scores = []
    unrelated_scores = []

    # Test related pairs
    for w1, w2 in related_words_pairs:
        w1_lower, w2_lower = w1.lower(), w2.lower()
        if w1_lower in word_to_embedding and w2_lower in word_to_embedding:
            sim = cosine_similarity(word_to_embedding[w1_lower], word_to_embedding[w2_lower])
            related_scores.append(sim)

    # Test unrelated pairs
    for w1, w2 in unrelated_words_pairs:
        w1_lower, w2_lower = w1.lower(), w2.lower()
        if w1_lower in word_to_embedding and w2_lower in word_to_embedding:
            sim = cosine_similarity(word_to_embedding[w1_lower], word_to_embedding[w2_lower])
            unrelated_scores.append(sim)

    # Calculate statistics
    if related_scores and unrelated_scores:
        related_mean = stats.mean(related_scores)
        related_var = stats.variance(related_scores) if len(related_scores) > 1 else 0
        unrelated_mean = stats.mean(unrelated_scores)
        unrelated_var = stats.variance(unrelated_scores) if len(unrelated_scores) > 1 else 0

        print("\n=== Results ===")
        print(f"Related pairs mean: {related_mean:.6f}")
        print(f"Related pairs variance: {related_var:.6f}")
        print(f"Unrelated pairs mean: {unrelated_mean:.6f}")
        print(f"Unrelated pairs variance: {unrelated_var:.6f}")

        if related_mean > unrelated_mean:
            print("✓ SUCCESS: Related pairs have higher similarity")
        else:
            print("✗ FAILURE: Unrelated pairs have higher similarity")
    else:
        print("Not enough data to calculate statistics")
    """